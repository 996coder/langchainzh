 Petals
 [#](#petals "Permalink to this headline")
===================================================
`Petals`
 runs 100B+ language models at home, BitTorrent-style.
 This notebook goes over how to use Langchain with
 [Petals](https://github.com/bigscience-workshop/petals) 
 .
 Install petals
 [#](#install-petals "Permalink to this headline")
-------------------------------------------------------------------
 The
 `petals`
 package is required to use the Petals API. Install
 `petals`
 using
 `pip3
 install
 petals`
 .
```
!pip3 install petals
```
 Imports
 [#](#imports "Permalink to this headline")
-----------------------------------------------------
```
import os
from langchain.llms import Petals
from langchain import PromptTemplate, LLMChain
```
 Set the Environment API Key
 [#](#set-the-environment-api-key "Permalink to this headline")
---------------------------------------------------------------------------------------------
 Make sure to get
 [your API key](https://huggingface.co/docs/api-inference/quicktour#get-your-api-token) 
 from Huggingface.
```
from getpass import getpass
HUGGINGFACE\_API\_KEY = getpass()
```
```
os.environ["HUGGINGFACE\_API\_KEY"] = HUGGINGFACE\_API\_KEY
```
 Create the Petals instance
 [#](#create-the-petals-instance "Permalink to this headline")
-------------------------------------------------------------------------------------------
 You can specify different parameters such as the model name, max new tokens, temperature, etc.
```
# this can take several minutes to download big files!
llm = Petals(model\_name="bigscience/bloom-petals")
```
```
Downloading:   1%|‚ñè                        | 40.8M/7.19G [00:24<15:44, 7.57MB/s]
```
 Create a Prompt Template
 [#](#create-a-prompt-template "Permalink to this headline")
---------------------------------------------------------------------------------------
 We will create a prompt template for Question and Answer.
```
template = """Question: {question}
Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input\_variables=["question"])
```
 Initiate the LLMChain
 [#](#initiate-the-llmchain "Permalink to this headline")
---------------------------------------------------------------------------------
```
llm\_chain = LLMChain(prompt=prompt, llm=llm)
```
 Run the LLMChain
 [#](#run-the-llmchain "Permalink to this headline")
-----------------------------------------------------------------------
 Provide a question and run the LLMChain.
```
question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"
llm\_chain.run(question)
```