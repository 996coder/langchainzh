 SageMaker Endpoint Embeddings
 [#](#sagemaker-endpoint-embeddings "Permalink to this headline")
=================================================================================================
 Letâ€™s load the SageMaker Endpoints Embeddings class. The class can be used if you host, e.g. your own Hugging Face model on SageMaker.
 For instructions on how to do this, please see
 [here](https://www.philschmid.de/custom-inference-huggingface-sagemaker) 
 .
 **Note** 
 : In order to handle batched requests, you will need to adjust the return line in the
 `predict\_fn()`
 function within the custom
 `inference.py`
 script:
 Change from
`return
 {"vectors":
 sentence\_embeddings[0].tolist()}`
 to:
`return
 {"vectors":
 sentence\_embeddings.tolist()}`
 .
```
!pip3 install langchain boto3
```
```
from typing import Dict, List
from langchain.embeddings import SagemakerEndpointEmbeddings
from langchain.llms.sagemaker\_endpoint import ContentHandlerBase
import json
class ContentHandler(ContentHandlerBase):
    content\_type = "application/json"
    accepts = "application/json"
    def transform\_input(self, inputs: list[str], model\_kwargs: Dict) -> bytes:
        input\_str = json.dumps({"inputs": inputs, \*\*model\_kwargs})
        return input\_str.encode('utf-8')
    def transform\_output(self, output: bytes) -> List[List[float]]:
        response\_json = json.loads(output.read().decode("utf-8"))
        return response\_json["vectors"]
content\_handler = ContentHandler()
embeddings = SagemakerEndpointEmbeddings(
    # endpoint\_name="endpoint-name", 
    # credentials\_profile\_name="credentials-profile-name", 
    endpoint\_name="huggingface-pytorch-inference-2023-03-21-16-14-03-834", 
    region\_name="us-east-1", 
    content\_handler=content\_handler
)
```
```
query\_result = embeddings.embed\_query("foo")
```
```
doc\_results = embeddings.embed\_documents(["foo"])
```
```
doc\_results
```